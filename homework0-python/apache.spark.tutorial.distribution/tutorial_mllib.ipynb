{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLlib Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IPython notebooks consist of multiple \"cells\", which can contain text (markdown) or code. You can run a single cell, multiple cells at a time, or every cellin the file. IPython also gives you the ability to export the notebook into different formats, such as a python file, a PDF file, an HTML file, etc.\n",
    "\n",
    "Spend a few minutes and make yourself familiar with the IPython interface, it should be pretty straight forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Machine Learning Library contains common machine learning algorithms and utilties. A summary of these features can be found [here](http://spark.apache.org/docs/latest/mllib-guide.html).\n",
    "\n",
    "In this tutorial, we will explore [collaborative filtering](http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html) on a small example problem. For your submission, you must turn in a PDF of this IPython notebook, but fully completed with any missing code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will go through an exercise based on the example from the collaborative filtering page. The first step is to import the MLlib library and whatever functions we want with this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS, Rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Input Data\n",
    "Next, we must load the data. There are some example datasets that come with Spark by default. Example data related to machine learning in particular is located in the `$SPARK_HOME/data/mllib` directory. For this part, we will be working with the `$SPARK_HOME/data/mllib/als/test.data` file. This is a small dataset, so it is easy to see what is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = sc.textFile(\"spark-1.6.2-bin-hadoop2.6/data/mllib/als/test.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though, we have the environment `$SPARK_HOME` defined, but it can't be used here. You must specify the full path, or the relative path based off where the IPython file is located.\n",
    "\n",
    "The textFile command will create an RDD where each element is a line of the input file. In the below cell, write some code to (1) print the number of elements and (2) print the fifth element. Print your result in a single line with the format: \"There are X elements. The fifth element is: Y\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the Input Data\n",
    "This data isn't in a great format, since each element is in the RDD is currently a string. However, we will assume that the first column of the string represents a user ID, the second column represents a product ID, and the third column represents a user-specified rating of that product.\n",
    "\n",
    "In the below cell, write a function that takes a string (that has the same format as lines in this file) as input and returns a tuple where the first and second elements are ints and the third element is a float. **Call your function `parser`.**\n",
    "\n",
    "We will then use this function to transform the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratings = data.map(parser).map(lambda l: Rating(*l))\n",
    "ratings.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look like the following:\n",
    "```\n",
    "[Rating(user=1, product=1, rating=5.0),\n",
    " Rating(user=1, product=2, rating=1.0),\n",
    " Rating(user=1, product=3, rating=5.0),\n",
    " Rating(user=1, product=4, rating=1.0),\n",
    " Rating(user=2, product=1, rating=5.0),\n",
    " Rating(user=2, product=2, rating=1.0),\n",
    " Rating(user=2, product=3, rating=5.0),\n",
    " Rating(user=2, product=4, rating=1.0),\n",
    " Rating(user=3, product=1, rating=1.0),\n",
    " Rating(user=3, product=2, rating=5.0),\n",
    " Rating(user=3, product=3, rating=1.0),\n",
    " Rating(user=3, product=4, rating=5.0),\n",
    " Rating(user=4, product=1, rating=1.0),\n",
    " Rating(user=4, product=2, rating=5.0),\n",
    " Rating(user=4, product=3, rating=1.0),\n",
    " Rating(user=4, product=4, rating=5.0)]\n",
    " ```\n",
    " \n",
    "If it doesn't, then you did something wrong! If it does match, then you are ready to move to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and Running the Model\n",
    "\n",
    "Now we are ready to build the actual recommendation model using the Alternating Least Squares algorithm. The documentation can be found [here](http://spark.apache.org/docs/1.5.2/api/python/pyspark.mllib.html#module-pyspark.mllib.recommendation), and the papers the algorithm is based on are linked off the [collaborative filtering page](http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rank = 10\n",
    "numIterations = 10\n",
    "model = ALS.train(ratings, rank, numIterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's define some test data\n",
    "testdata = ratings.map(lambda p: (p[0], p[1]))\n",
    "\n",
    "# Running the model on all possible user->product predictions\n",
    "predictions = model.predictAll(testdata)\n",
    "predictions.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the Model Output\n",
    "\n",
    "This result is not really in a nice format. Write some code that will transform the RDD so that each element is a user ID and a dictionary of product->rating pairs. Note that for the a Ratings object (which is what the elements of the RDD are), you can access the different fields by via the `.user`, `.product`, and `.rating` variables. For example, `predictions.take(1)[0].user`. \n",
    "\n",
    "Call the new RDD userPredictions. It should look as follows (when using `userPredictions.collect()`):\n",
    "```\n",
    "[(4,\n",
    "  {1: 1.0011434289237737,\n",
    "   2: 4.996713610813412,\n",
    "   3: 1.0011434289237737,\n",
    "   4: 4.996713610813412}),\n",
    " (1,\n",
    "  {1: 4.996411869659315,\n",
    "   2: 1.0012037253934976,\n",
    "   3: 4.996411869659315,\n",
    "   4: 1.0012037253934976}),\n",
    " (2,\n",
    "  {1: 4.996411869659315,\n",
    "   2: 1.0012037253934976,\n",
    "   3: 4.996411869659315,\n",
    "   4: 1.0012037253934976}),\n",
    " (3,\n",
    "  {1: 1.0011434289237737,\n",
    "   2: 4.996713610813412,\n",
    "   3: 1.0011434289237737,\n",
    "   4: 4.996713610813412})]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model\n",
    "Now, lets calculate the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "userPredictions = predictions.map(lambda r: ((r[0],r[1]), r[2]))\n",
    "ratesAndPreds = ratings.map(lambda r: ((r[0],r[1]), r[2])).join(userPredictions)\n",
    "MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "print(\"Mean Squared Error = \" + str(MSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Reflections\n",
    "Why do you believe that this model achieved such a low error. Is there anything we did incorrectly for testing this model?\n",
    "\n",
    "```-----------------```\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
