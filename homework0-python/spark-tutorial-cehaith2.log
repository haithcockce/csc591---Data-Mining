get_ipython().magic(u'logstart "~/591/homework0-python/section3.log" append')
rdd_1 = sc.parallelize([1, 2, 3, 4, 5, 6, 7])
rdd_2 = sc.text('/opt/spark-1.6.2-bin-hadoop2.6/data/mllib/pic_data.txt')
rdd_2 = sc.textFile('/opt/spark-1.6.2-bin-hadoop2.6/data/mllib/pic_data.txt')
rdd = sc.parallelize([1, 2, 3])
rdd.collect()
rdd.count()
rdd.first()
rdd.take(5)
rdd.reduce(lambda x,y: x + y)
rdd.reduce(lambda x,y: x * y)
rdd_squared = rdd.map(lambda x: x**2)
rdd_squared.collect()
rdd.filter(lambda x: x > 0).collect()
rdd = sc.parallelize([1, 2, -3, 4, -5, 6, 7])
rdd.filter(lambda x: x > 0).collect()
rdd = sc.parallelize( [ ("a", 1), ("a", 2), ("b", 1) ] )
rdd_grouped = rdd.groupByKey()
rdd_grouped.map(lambda x: (x[0], list(x[1]))).collect()
rdd = sc.parallelize( [2, 3, 4] )
rdd.map(lambda x: range(1, x)).collect()
rdd.flatMapp(lambda x: range(1,x)).collect()
rdd.flatMap(lambda x: range(1,x)).collect()
text=sc.parallelize(["I stepped on a Corn Flake, now I am a Cereal Killer",    "Hello world inside hello world","Banana error",    "Not Lorem Ipsum","what a wonderful day","what up?"])
split_rdd = text.flatMap(lambda line: line.split(" "))
marked_rdd = split_rdd.map(lambda word: (word, 1))
counts = marked_rdd.reduceByKey(lambda a,b: a + b)
counts.collect()
get_ipython().magic(u'logstop')
get_ipython().magic(u'logstart "~/591/homework0-python/section4.log" append')
from pyspark.sql import SQLContext
sqlcontext = SQLContext(sc)
df = sqlContext.read.json('/opt/spark-1.6.2-bin-hadoop2.6/examples/src/main/resources/people.json')
df.show()
df.printSchema()
df.select("name").show()
df.select(df['name'], df['age'] + 1).show()
df.filter(df['age'] > 21).show()
df.groupBy("age").count().show()
df = sqlContext.sql("SELECT * FROM table")
df = sqlContext.sql("SELECT * FROM df")
get_ipython().magic(u'logstop')
get_ipython().magic(u'logstart "~/591/homework0-python/section5.log" append')
from graphframes.examples import Graphs
v = sqlContext.createDataFrame([
  ("a", "Alice", 34),
  ("b", "Bob", 36),
  ("c", "Charlie", 30),
  ("d", "David", 29),
  ("e", "Esther", 32),
  ("f", "Fanny", 36),
  ("g", "Gabby", 60)
], ["id", "name", "age"])
e = sqlContext.createDataFrame([
  ("a", "b", "friend"),
  ("b", "c", "follow"),
  ("c", "b", "follow"),
  ("f", "c", "follow"),
  ("e", "f", "follow"),
  ("e", "d", "friend"),
  ("d", "a", "friend"),
  ("a", "e", "friend")
], ["src", "dst", "relationship"])
from graphframes.examples import Graphs
g = Graphs(sqlContext).friends()
g.vertices.show()
g.edges.show()
vertexInDegrees = g.inDegrees
g.vertices.groupBy().min("age").show()
numFollows = g.edges.filter("relationship = 'follow'").count()
numFollows
vertexInDegrees
motifs = g.find("(a)-[e]->(b); (b)-[e2]->(a)")
motifs.show()
motifs.filter("b.age > 30").show()
from pyspark.sql.functions import col, lit, udf, when
from pyspark.sql.types import IntegerType
from graphframes.examples import Graphs
chain4 = g.find("(a)-[ab]->(b); (b)-[bc]->(c); (c)-[cd]->(d)")
sumFriends =  lambda cnt,relationship: when(relationship == "friend", cnt+1).otherwise(cnt)
condition =  reduce(lambda cnt,e: sumFriends(cnt, col(e).relationship), ["ab", "bc", "cd"], lit(0))
chainWith2Friends2 = chain4.where(condition >= 2)
chainWith2Friends2.show()
paths = g.find("(a)-[e]->(b)")  .filter("e.relationship = 'follow'")  .filter("a.age < b.age")
e2 = paths.select("e.src", "e.dst", "e.relationship")
g2 = GraphFrame(g.vertices, e2)
from graphframes import *
g2 = GraphFrame(g.vertices, e2)
paths = g.bfs("name = 'Esther'", "age < 32")
paths.show()
result = g.connectedComponents()
result.select("id", "component").orderBy("component").show()
result = g.stronglyConnectedComponents(maxIter=10)
result.select("id", "component").orderBy("component").show()
result = g.labelPropagation(maxIter=5)
result.select("id", "label").show()
g = Graphs(sqlContext).friends()  # Get example graph
results = g.pageRank(resetProbability=0.15, tol=0.01)
results.vertices.select("id", "pagerank").show()
results.edges.select("src", "dst", "weight").show()
results2 = g.pageRank(resetProbability=0.15, maxIter=10)
results3 = g.pageRank(resetProbability=0.15, maxIter=10, sourceId="a")
results = g.shortestPaths(landmarks=["a", "d"])
results.select("id", "distances").show()
results = g.triangleCount()
results.select("id", "count").show()
get_ipython().magic(u'logstop')
