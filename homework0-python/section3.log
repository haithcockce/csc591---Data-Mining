get_ipython().magic(u'logstart "~/591/homework0-python/section3.log" append')
rdd_1 = sc.parallelize([1, 2, 3, 4, 5, 6, 7])
rdd_2 = sc.text('/opt/spark-1.6.2-bin-hadoop2.6/data/mllib/pic_data.txt')
rdd_2 = sc.textFile('/opt/spark-1.6.2-bin-hadoop2.6/data/mllib/pic_data.txt')
rdd = sc.parallelize([1, 2, 3])
rdd.collect()
rdd.count()
rdd.first()
rdd.take(5)
rdd.reduce(lambda x,y: x + y)
rdd.reduce(lambda x,y: x * y)
rdd_squared = rdd.map(lambda x: x**2)
rdd_squared.collect()
rdd.filter(lambda x: x > 0).collect()
rdd = sc.parallelize([1, 2, -3, 4, -5, 6, 7])
rdd.filter(lambda x: x > 0).collect()
rdd = sc.parallelize( [ ("a", 1), ("a", 2), ("b", 1) ] )
rdd_grouped = rdd.groupByKey()
rdd_grouped.map(lambda x: (x[0], list(x[1]))).collect()
rdd = sc.parallelize( [2, 3, 4] )
rdd.map(lambda x: range(1, x)).collect()
rdd.flatMapp(lambda x: range(1,x)).collect()
rdd.flatMap(lambda x: range(1,x)).collect()
text=sc.parallelize(["I stepped on a Corn Flake, now I am a Cereal Killer",    "Hello world inside hello world","Banana error",    "Not Lorem Ipsum","what a wonderful day","what up?"])
split_rdd = text.flatMap(lambda line: line.split(" "))
marked_rdd = split_rdd.map(lambda word: (word, 1))
counts = marked_rdd.reduceByKey(lambda a,b: a + b)
counts.collect()
get_ipython().magic(u'logstop')
